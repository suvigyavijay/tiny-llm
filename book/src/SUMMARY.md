# LLM Serving in a Week

[Preface](./preface.md)
[Setting Up the Environment](./setup.md)

---

- [Week 1: Build Your Own Qwen2 From Scratch](week1-overview.md)
  - [Day 1: Attention and Multi-Head Attention](week1-01-attention.md)
  - [Day 2: Positional Encodings (RoPE)](week1-02-positional-encodings.md)
  - [Day 3: Grouped-Query Attention](week1-03-gqa.md)
  - [Day 4: RMSNorm and MLP](week1-04-rmsnorm-and-mlp.md)
  - [Day 5: Load Qwen2 Weights](week1-05-qwen2-model.md)
  - [Day 6: Generate a Response](week1-06-generate-response.md)
  - [Day 7: Sampling](week1-07-sampling-prepare.md)
- [Week 2: Build Your Own vLLM](week2-overview.md)
  - [Day 1: Key-Value Cache](week2-01-kv-cache.md)
  - [Day 2: Quantized Matmul and Linear - CPU](week2-02-quantized-matmul-cpu.md)
  - [Day 3: Quantized Matmul and Linear - GPU](week2-03-quantized-matmul-gpu.md)
  - [Day 4: Flash Attention 2 - CPU](week2-04-flash-attention-cpu.md)
  - [Day 5: Flash Attention 2 - GPU](week2-05-flash-attention-gpu.md)
  - [Day 6: Continuous Batching](week2-06-continuous-batching.md)
  - [Day 7: Chunked Prefill](week2-07-chunked-prefill.md)
- [Week 3: Advanced Topics](week3-overview.md)
  - [Day 1: Paged Attention - Part 1](week3-01-paged-attention-part1.md)
  - [Day 2: Paged Attention - Part 2](week3-02-paged-attention-part2.md)
  - [Day 3: MoE (Mixture of Experts)](week3-03-moe.md)
  - [Day 4: Speculative Decoding](week3-04-speculative-decoding.md)
  - [Day 5: RAG Pipeline](week3-05-rag-pipeline.md)
  - [Day 6: AI Agent / Tool Calling](week3-06-agent-tool-calling.md)
  - [Day 7: Long Context](week3-07-long-context.md)

---
[Glossary](glossary.md)
