# LLM Serving in a Week

[Preface](./preface.md)
[Setting Up the Environment](./setup.md)

---

- [Week 1: From Matmul to Text](./week1-overview.md)
    - [Attention and Multi-Head Attention](./week1-01-attention.md)
    - [Positional Encodings and RoPE](./week1-02-positional-encodings.md)
    - [Grouped/Multi Query Attention](./week1-03-gqa.md)
    - [RMSNorm and MLP](./week1-04-rmsnorm-and-mlp.md)
    - [The Qwen2 Model](./week1-05-qwen2-model.md)
    - [Generating the Response](./week1-06-generate-response.md)
    - [Sampling and Preparing for Week 2](./week1-07-sampling-prepare.md)
    <!--
    - [Attention and Multi-Head Attention](./week1-01-attention.md)
    - [Positional Embeddings and RoPE](./week1-02-positional-embeddings.md)
    - [Grouped/Multi Query Attention](./week1-03-gqa.md)
    - [Multilayer Perceptron Layer and Transformer](./week1-04-mlp-transformer.md)
    - [Wiring the Qwen2 Model](./week1-05-model-1.md)
    - [Loading the Model](./week1-06-model-2.md)
    - [Generating the Response](./week1-07-generate.md)
    -->

- [Week 2: Optimizing](./week2-overview.md)
    - [Key-Value Cache](./week2-01-kv-cache.md)
    - [Quantized Matmul and Linear - CPU](./week2-02-quantized-cpu.md)
    - [Quantized Matmul and Linear - GPU](./week2-03-quantized-gpu.md)
    - [Flash Attention 2 - CPU](./week2-04-flash-attention-cpu.md)
    - [Flash Attention 2 - GPU](./week2-05-flash-attention-gpu.md)
    - [Continuous Batching](./week2-06-continuous-batching.md)
    - [Chunked Prefill](./week2-07-chunked-prefill.md)

- [Week 3: Advanced Serving](./week3-overview.md)
    - [Paged Attention - Part 1](./week3-01-paged-attention-1.md)
    - [Paged Attention - Part 2](./week3-02-paged-attention-2.md)
    - [Mixture of Experts (MoE)](./week3-03-moe.md)
    - [Speculative Decoding](./week3-04-speculative-decoding.md)
    - [RAG Pipeline](./week3-05-rag.md)
    - [AI Agent & Tool Calling](./week3-06-agent.md)
    - [Long Context Handling](./week3-07-long-context.md)

---

[Glossary Index](./glossary.md)
