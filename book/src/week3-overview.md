# Week 3: Advanced Topics

In this final week, we will explore some of the most advanced techniques in LLM serving. These techniques are at the forefront of LLM research and are essential for building powerful and versatile AI systems.

## What We will Cover

* **Paged Attention**: Build upon the data structures from Week 2 to implement a simplified paged attention kernel.
* **Mixture of Experts (MoE)**: Implement a MoE layer to increase the capacity of the model without a proportional increase in computational cost.
* **Speculative Decoding**: Implement speculative decoding to accelerate inference by using a smaller, faster draft model.
* **RAG Pipeline**: Implement a Retrieval-Augmented Generation (RAG) pipeline to enable the model to access external knowledge.
* **AI Agent / Tool Calling**: Implement a simple AI agent that can interact with external tools.
* **Long Context**: Implement a sliding window attention mechanism to handle long input sequences.

## What We will Not Cover

* **Production-Ready Implementations**: The implementations in this week are simplified for educational purposes. A production-ready system would require more advanced and optimized implementations.
* **Training and Fine-Tuning**: This course focuses on inference. Training and fine-tuning are out of the scope of this course.

## The Future of LLM Serving

The field of LLM serving is rapidly evolving. The techniques you've learned in this course will provide you with a strong foundation for understanding and contributing to this exciting field.

**ðŸ“š Readings**

- [The Future of LLM Serving is Bright](https://www.anyscale.com/blog/the-future-of-llm-serving-is-bright-and-its-not-just-about-the-gpu)
- [DeepSpeed-Inference: Enabling Efficient Inference of Large Language Models](https://www.microsoft.com/en-us/research/blog/deepspeed-inference-enabling-efficient-inference-of-large-language-models/)

{{#include copyright.md}}
